{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "ksvm = SVC(kernel = 'poly', random_state = 0)\n",
    "#‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "\n",
    "ksvm_bow = ksvm.fit(cv_train_text, train_sentiments)\n",
    "# Predicting the Test set results\n",
    "ksvm_bow_predict = ksvm.predict(cv_test_text)\n",
    "\n",
    "ksvm_tfidf = ksvm.fit(tv_train_text, train_sentiments)\n",
    "# Predicting the Test set results\n",
    "ksvm_tfidf_predict = ksvm.predict(tv_test_text)\n",
    "\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,ksvm_tfidf_predict,labels=[1,0])\n",
    "conf_matrices[\"ksvm_tfidf\"] = cm_tfidf\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4)  # for label size\n",
    "sns.heatmap(cm_tfidf, annot=True, annot_kws={\"size\": 16}, fmt='g')  # fmt='g' is a format option to suppress scientific notation\n",
    "\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('TF-IDF Confusion Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,ksvm_bow_predict,labels=[1,0])\n",
    "conf_matrices[\"ksvm_bow\"] = cm_bow\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4)  # for label size\n",
    "sns.heatmap(cm_bow, annot=True, annot_kws={\"size\": 16}, fmt='g')  # fmt='g' is a format option to suppress scientific notation\n",
    "\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('BoW Confusion Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([(\"X\", OneHotEncoder(dtype = int), [1])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(cv_train_text, train_sentiments, batch_size = 20, epochs = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "cm_tfidf = confusion_matrix(y_test, y_pred)\n",
    "conf_matrices[\"ann_tfidf\"] = cm_tfidf\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm_bow = confusion_matrix(y_test, y_pred)\n",
    "conf_matrices[\"ann_bow\"] = cm_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing these four models, we can see from the confusions matrixes that in every case we have fewer False Positives by using BoW than TFIDF so we will only work with BoW from this point onwards.\n",
    "\n",
    "We can also see that Kernel SVM has the highest amount of FP, so we will only work with the other 3 models to tune their hyperparameters and build better models. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
